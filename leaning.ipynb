{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning how to build models for multiclass predictions\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MulticlassClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MulticlassClassifier, self).__init__()\n",
    "        \n",
    "        # Layer 1: Input ‚Üí Hidden\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Layer 2: Hidden ‚Üí Hidden\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Layer 3: Hidden ‚Üí Output\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        # No softmax here! CrossEntropyLoss includes it\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.relu1(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x  # Raw logits, not probabilities yet\n",
    "\n",
    "# Create model\n",
    "model = MulticlassClassifier(input_size=3072,  # 32x32x3 flattened\n",
    "                              hidden_size=128, \n",
    "                              num_classes=3)    # cat, dog, bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d09ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93b587",
   "metadata": {},
   "source": [
    "Too high (0.1): Loss jumps around, never converges, might diverge\n",
    "\n",
    "Too low (0.00001): Training takes forever, gets stuck in local minima\n",
    "\n",
    "Just right (0.001-0.0001): Steady improvement, reaches good solution\n",
    "\n",
    "How to choose:\n",
    "\n",
    "Start with 0.001 (Adam) or 0.01 (SGD)\n",
    "\n",
    "If loss oscillates wildly ‚Üí decrease by 10x\n",
    "\n",
    "If loss barely moves after 10 epochs ‚Üí increase by 2-3x\n",
    "\n",
    "Use learning rate scheduling: Start high, decrease over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "# Reduces LR by 50% if validation loss doesn't improve for 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a16525",
   "metadata": {},
   "source": [
    "Epoch 1-20: LR = 0.001 ‚Üí Loss drops from 2.5 to 0.8\n",
    "\n",
    "Epoch 21-40: LR = 0.0005 ‚Üí Loss drops from 0.8 to 0.4\n",
    "\n",
    "Epoch 41+: LR = 0.00025 ‚Üí Loss fine-tunes to 0.3\n",
    "\n",
    "\n",
    "2. Batch Size - Memory vs Stability Trade-off\n",
    "\n",
    "What it is: Number of examples processed before updating weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece1bb6",
   "metadata": {},
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "**Why it matters**:\n",
    "- **Small batch (8-16)**: \n",
    "  - ‚úÖ More updates per epoch (faster learning)\n",
    "  - ‚úÖ Better generalization (noise helps escape local minima)\n",
    "  - ‚ùå Slower per epoch (more iterations)\n",
    "  - ‚ùå Noisy gradients (jumpy training)\n",
    "\n",
    "- **Large batch (128-256)**:\n",
    "  - ‚úÖ Faster per epoch (GPU efficient)\n",
    "  - ‚úÖ Stable gradients (smooth training curve)\n",
    "  - ‚ùå Fewer updates per epoch\n",
    "  - ‚ùå Can overfit to training data\n",
    "\n",
    "**How to choose**:\n",
    "1. Start with **32** (good default)\n",
    "2. Increase until GPU memory is full (use `nvidia-smi` to check)\n",
    "3. If you increase batch size by 2x, increase learning rate by ‚àö2\n",
    "\n",
    "**Memory calculation**:\n",
    "```\n",
    "GPU Memory = Batch Size √ó Model Size √ó Gradient Size\n",
    "If batch 32 uses 4GB ‚Üí batch 64 uses ~8GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe85bd5",
   "metadata": {},
   "source": [
    "Too few (10): Model hasn't learned enough (underfitting)\n",
    "Too many (500): Model memorizes training data (overfitting)\n",
    "\n",
    "How to choose: Use early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ecf6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss: Keeps decreasing\n",
    "# Validation loss: Decreases then plateaus or increases\n",
    "# Stop when validation loss stops improving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What it is: Depth (layers) and width (neurons per layer)\n",
    "\n",
    "# 2 hidden layers, 128 neurons each\n",
    "self.fc1 = nn.Linear(input_size, 128)\n",
    "self.fc2 = nn.Linear(128, 128)\n",
    "self.fc3 = nn.Linear(128, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c481de",
   "metadata": {},
   "source": [
    "More layers: Can learn hierarchical features (edges ‚Üí shapes ‚Üí objects)\n",
    "\n",
    "More neurons: More representational capacity\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "Simple data (tabular): 2-3 layers, 64-256 neurons\n",
    "\n",
    "Images: Use CNNs instead (10-100+ layers)\n",
    "\n",
    "Text: Use Transformers (12-96 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8357a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If training accuracy is low (< 80%), your model lacks capacity\n",
    "# ‚Üí Add more neurons or layers\n",
    "\n",
    "# If training accuracy is high (> 95%) but validation is low (< 70%)\n",
    "# ‚Üí Model is too large, reduce capacity or add regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a20039",
   "metadata": {},
   "source": [
    "# Dropout - Preventing Overfitting\n",
    "What it is: Randomly \"turns off\" neurons during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be96a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.dropout = nn.Dropout(0.5)  # 50% of neurons dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ef8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# === SETUP ===\n",
    "model = MulticlassClassifier(input_size=3072, hidden_size=128, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === TRAINING LOOP ===\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # === TRAINING PHASE ===\n",
    "    model.train()  # Enable dropout\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # inputs: [32, 3072] - batch of 32 images\n",
    "        # targets: [32] - batch of 32 labels (0, 1, or 2)\n",
    "        \n",
    "        # === FORWARD PASS ===\n",
    "        outputs = model(inputs)  # [32, 3] - 3 scores per image\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # === BACKWARD PASS ===\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        loss.backward()        # Compute new gradients\n",
    "        optimizer.step()       # Update weights\n",
    "        \n",
    "        # === TRACK METRICS ===\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with highest score\n",
    "        train_total += targets.size(0)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # === VALIDATION PHASE ===\n",
    "    model.eval()  # Disable dropout\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradients (saves memory)\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # === COMPUTE AVERAGES ===\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    # === LEARNING RATE SCHEDULING ===\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # === EARLY STOPPING ===\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'‚úì Model saved at epoch {epoch+1}')\n",
    "    \n",
    "    # === LOGGING ===\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "    print(f'  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "    print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üìà What You'll See During Training**\n",
    "\n",
    "**Healthy Training**:\n",
    "```\n",
    "Epoch [1/100]\n",
    "  Train Loss: 1.0986 | Train Acc: 33.52%  (random guessing)\n",
    "  Val Loss: 1.0891 | Val Acc: 34.20%\n",
    "\n",
    "Epoch [10/100]\n",
    "  Train Loss: 0.6523 | Train Acc: 72.80%  (learning!)\n",
    "  Val Loss: 0.7124 | Val Acc: 69.50%\n",
    "\n",
    "Epoch [50/100]\n",
    "  Train Loss: 0.2341 | Train Acc: 91.20%  (good fit)\n",
    "  Val Loss: 0.3892 | Val Acc: 86.10%\n",
    "\n",
    "Epoch [100/100]\n",
    "  Train Loss: 0.1123 | Train Acc: 96.40%  (converged)\n",
    "  Val Loss: 0.3654 | Val Acc: 87.30%\n",
    "```\n",
    "\n",
    "**Overfitting**:\n",
    "```\n",
    "Epoch [100/100]\n",
    "  Train Loss: 0.0234 | Train Acc: 99.80%  ‚Üê Perfect on training\n",
    "  Val Loss: 1.2341 | Val Acc: 72.10%      ‚Üê Bad on validation\n",
    "  \n",
    "‚Üí Model memorized training data!\n",
    "‚Üí Solutions: Add dropout, reduce model size, get more data\n",
    "```\n",
    "\n",
    "**Underfitting**:\n",
    "```\n",
    "Epoch [100/100]\n",
    "  Train Loss: 0.8234 | Train Acc: 68.20%  ‚Üê Can't even fit training data\n",
    "  Val Loss: 0.8456 | Val Acc: 67.10%\n",
    "  \n",
    "‚Üí Model too simple or learning rate too low\n",
    "‚Üí Solutions: Add layers/neurons, increase LR, train longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f2c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c496e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
